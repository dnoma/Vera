import type { LegalBenchEvalRun } from './runner.js';

export function legalBenchMarkdownReport(run: LegalBenchEvalRun): string {
  const lines: string[] = [];
  lines.push('**LegalBench Evaluation**');
  lines.push('');
  lines.push(`- Dataset: \`${run.datasetPath}\``);
  lines.push(`- Split: \`${run.split}\``);
  lines.push(`- Prompt mode: \`${run.promptMode}\``);
  lines.push(`- Normalize outputs: \`${run.normalizeOutputs}\``);
  lines.push(`- Concurrency: \`${run.concurrency}\``);
  lines.push(`- Model: \`${run.openai.model}\` (temperature=${run.openai.temperature})`);
  lines.push(`- Examples: ${run.overall.predicted}/${run.overall.total}`);
  lines.push(
    `- Macro average score (across tasks): ${
      run.overall.macroScore === null ? 'n/a' : run.overall.macroScore.toFixed(4)
    }`
  );
  lines.push('');
  lines.push('| Task | Reasoning | Type | Metric | Score | Predicted/Total |');
  lines.push('|---|---|---|---|---:|---:|');
  for (const t of run.taskSummaries) {
    const score = t.score === null ? 'n/a' : t.score.toFixed(4);
    lines.push(
      `| \`${t.task}\` | ${t.reasoningType} | ${t.taskType} | \`${t.metric}\` | ${score} | ${t.predicted}/${t.total} |`
    );
  }
  lines.push('');
  lines.push('_Generated by `npm run eval -- --dataset legalbench`._');
  return lines.join('\n');
}
