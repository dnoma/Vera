import type { EvalRun, EvalSummary } from './types.js';

function pct(value: number | null): string {
  if (value === null) return 'n/a';
  return `${(value * 100).toFixed(1)}%`;
}

function row(label: string, a: string, b: string): string {
  return `| ${label} | ${a} | ${b} |`;
}

export function summarize(run: EvalRun): readonly EvalSummary[] {
  const byMethod = new Map<string, EvalSummary>();

  for (const method of run.methods) {
    const results = run.results.filter(r => r.method === method);
    const total = results.length;
    const predictedResults = results.filter(r => r.predicted !== null);
    const predicted = predictedResults.length;
    const correct = predictedResults.filter(r => r.predicted === r.label).length;

    const citationsChecked = results.reduce((sum, r) => sum + r.citationsChecked, 0);
    const citationsMatched = results.reduce((sum, r) => sum + r.citationsMatched, 0);

    const accuracy = predicted === 0 ? null : correct / predicted;
    const citationMatchRate = citationsChecked === 0 ? null : citationsMatched / citationsChecked;

    const schemaChecks = results
      .map(r => r.schemaValidation)
      .filter((v): v is NonNullable<typeof v> => v !== undefined);
    const schemaPassRate =
      schemaChecks.length === 0
        ? null
        : schemaChecks.filter(v => v.valid).length / schemaChecks.length;

    const frameworkChecks = results
      .map(r => r.frameworkValidation)
      .filter((v): v is NonNullable<typeof v> => v !== undefined);
    const frameworkValidRate =
      frameworkChecks.length === 0
        ? null
        : frameworkChecks.filter(v => v.valid).length / frameworkChecks.length;

    const contestabilityChecks = results
      .flatMap(r => r.contestability ?? [])
      .filter(c => c.property === 'P1_pro_increase' || c.property === 'P1_con_increase');
    const contestabilityHoldRate =
      contestabilityChecks.length === 0
        ? null
        : contestabilityChecks.filter(c => c.holds).length / contestabilityChecks.length;

    byMethod.set(method, {
      method,
      total,
      predicted,
      accuracy,
      citationMatchRate,
      schemaPassRate,
      frameworkValidRate,
      contestabilityHoldRate,
    });
  }

  return run.methods.map(m => byMethod.get(m)!);
}

export function markdownReport(run: EvalRun): string {
  const summaries = run.summaries;
  const baseline = summaries.find(s => s.method === 'baseline');
  const qbaf = summaries.find(s => s.method === 'qbaf');

  const bAcc = baseline ? pct(baseline.accuracy) : 'n/a';
  const qAcc = qbaf ? pct(qbaf.accuracy) : 'n/a';

  const bFaith = baseline ? pct(baseline.citationMatchRate) : 'n/a';
  const qFaith = qbaf ? pct(qbaf.citationMatchRate) : 'n/a';

  const qSchema = qbaf ? pct(qbaf.schemaPassRate) : 'n/a';
  const qContest = qbaf ? pct(qbaf.contestabilityHoldRate) : 'n/a';

  const header = [
    `**CUAD v1 Evaluation**`,
    '',
    `- Dataset: \`${run.datasetPath}\``,
    `- Model: \`${run.openai.model}\` (temperature=${run.openai.temperature})`,
    `- Contracts: ${run.contractCount}, Categories: ${run.categoryCount}`,
    '',
    '| Metric | Baseline (Linear JSON) | QBAF (Graph + DF-QuAD) |',
    '|---|---:|---:|',
    row('Accuracy', bAcc, qAcc),
    row('Citation quote match rate', bFaith, qFaith),
    row('QBAF: schema/framework pass rate', 'n/a', qSchema),
    row('QBAF: contestability checks hold', 'n/a', qContest),
    '',
    '_Generated by `npm run eval`._',
  ].join('\n');

  return header;
}
